{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UGnQaGiuSBGe",
        "RIPFLdmNpT1F",
        "F8thIU77sdA5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount the google drive to import the model configuration"
      ],
      "metadata": {
        "id": "UGnQaGiuSBGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to mount the google drive\n",
        "def mountDrive(force_remount=True):\n",
        "    from google.colab import drive\n",
        "    print('drive_filepath=\"drive/My Drive/\"')\n",
        "    return drive.mount('/content/drive', force_remount=force_remount)\n",
        "\n",
        "mountDrive()"
      ],
      "metadata": {
        "id": "RIRt5xaGS69w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7c7f30-014c-4552-dcec-fbc150a59c5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive_filepath=\"drive/My Drive/\"\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing and Importing libraries"
      ],
      "metadata": {
        "id": "2FFykJKV1ke_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YF1rbQRXpON3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "#installing the required libraries\n",
        "\n",
        "!pip install selenium\n",
        "!pip install lxml\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#installing the required libraries\n",
        "\n",
        "#following libraries are mainly used for web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "\n",
        "#following libraries are mainly used for text preprocessing\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#following libraries are mainly used for text classification\n",
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "#following libraries are mainly used for cosine similarity calculation\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk import sent_tokenize\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "IE5_1mEqpgYS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "#to calculate execution time of each cell\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "dUdaikIQ1wHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcd9758-02e0-4d4e-c5b6-deefa5789292"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 645 Âµs (started: 2022-11-20 04:46:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scraping the privacy policy"
      ],
      "metadata": {
        "id": "RIPFLdmNpT1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to scrape the policy using headless browser and beautiful soup\n",
        "\n",
        "def scrapeUrl(url):\n",
        "\n",
        "  #setting the configuration of the headless browser \n",
        "  options = Options()\n",
        "  options.add_argument(\"--window-size=1920,1200\")\n",
        "  options.add_argument('--headless')\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "  #to avoid loading images\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
        "  \n",
        "  #importing the browser and scraping the webpage\n",
        "  driver = webdriver.Chrome('chromedriver',chrome_options=options)\n",
        "  driver.get(url) \n",
        "\n",
        "  #parcing the scraped data into soup\n",
        "  soup = BeautifulSoup(driver.page_source, 'lxml') \n",
        "\n",
        "  #to store the raw data as list of segments\n",
        "  rawPageData = []\n",
        "\n",
        "  #to find all the content under <p> and <li> tags\n",
        "  for data in soup.find_all(['p','li']): \n",
        "    rawPageData.append(data.get_text())\n",
        "  \n",
        "  return rawPageData "
      ],
      "metadata": {
        "id": "ISEUI-tlp51g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-processing the scraped data"
      ],
      "metadata": {
        "id": "F8thIU77sdA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''PriBERT accepts complete sentences therfore minimum pre-process is required.\n",
        "   The following function removed duplicates and sentences with less than two words.'''\n",
        "\n",
        "def minPreProcess(inputData):\n",
        "  data = []\n",
        "  [data.append(x) for x in inputData if x not in data]\n",
        "\n",
        "  clData = []\n",
        "  for element in data:\n",
        "    if element.strip().count(\" \") > 2:\n",
        "      clData.append(element)\n",
        "  \n",
        "  return clData"
      ],
      "metadata": {
        "id": "-K6aZj4E5zgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Classification"
      ],
      "metadata": {
        "id": "lUgUZXTVTpYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''To store the output of PrivBERT as a dictionary. \n",
        "   The key represents the privacy labels and value is a string of classified segments'''\n",
        "\n",
        "def storeAsDict(result):\n",
        "  \n",
        "  #mapping of ids to privacy labels\n",
        "  id2Label = {0: 'Data Retention', 1: 'Data Security', 2: 'Do Not Track', 3: 'First Party Collection/Use',\n",
        "                     4: 'International and Specific Audiences', 5: 'Introductory/Generic', 6: 'Policy Change',\n",
        "                     7: 'Practice not covered', 8:'Privacy contact information', 9: 'Third Party Sharing/Collection',\n",
        "                     10: 'User Access, Edit and Deletion', 11: 'User Choice/Control'}\n",
        "  \n",
        "  #to store the output\n",
        "  joinedResult= dict()\n",
        "\n",
        "  for key in result.keys():\n",
        "    temp = '.'.join(map(str, result[key]))\n",
        "    print(temp)\n",
        "    joinedResult[id2Label[key]] = temp\n",
        "    \n",
        "  return joinedResult"
      ],
      "metadata": {
        "id": "JiKNNW6IVkaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A dummy customization unit with four user types and each containing 5 ids of privacy aspects\n",
        "\n",
        "def customizer(select):\n",
        "  userOptions = dict()\n",
        "  userOptions[\"old male\"] = [6,3,9,11,1]\n",
        "  userOptions[\"young male\"] = [7,1,4,5,10]\n",
        "  userOptions[\"young female\"] = [7,6,1,4,8]\n",
        "  userOptions[\"old female\"] = [9,3,2,11,1]\n",
        "\n",
        "  return userOptions[select]"
      ],
      "metadata": {
        "id": "aGqGZUqKW2kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Main function to perform the classification of the privacy policy\n",
        "\n",
        "def classificationModel(datatoClassify, userType):\n",
        "  # retreive the saved model \n",
        "  \n",
        "  #path to the saved configuration of the trained PrivBERT\n",
        "  modelDir = '/content/drive/MyDrive/Colab Notebooks/models/PolicyInterpreterFullSample'\n",
        "  \n",
        "  #no of labes is equal to the number of privacy aspects for which the model was trained\n",
        "  numLabels = 12\n",
        "    \n",
        "  #to load the configuration of the trained PrivBERT\n",
        "  checkpoint = \"mukund/privbert\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  # For cleaner label outputs\n",
        "  id2Label = {0: 'Data Retention', 1: 'Data Security', 2: 'Do Not Track', 3: 'First Party Collection/Use',\n",
        "              4: 'International and Specific Audiences', 5: 'Introductory/Generic', 6: 'Policy Change',\n",
        "              7: 'Practice not covered', 8:'Privacy contact information', 9: 'Third Party Sharing/Collection',\n",
        "              10: 'User Access, Edit and Deletion', 11: 'User Choice/Control'}\n",
        "             \n",
        "  #saving the index of each label\n",
        "  label2id = {val: key for key, val in id2Label.items()}\n",
        "\n",
        "  '''the sequence classification head of the tranformer library is used to import the PrivBERT model \n",
        "    with mappings of the label'''\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "      modelDir, num_labels=numLabels, id2label=id2Label, label2id=label2id, local_files_only=True)\n",
        "  \n",
        "  #to provide customization of the privacy policy\n",
        "  user = customizer(userType)\n",
        "  \n",
        "  #to save the output\n",
        "  result = dict()\n",
        "  \n",
        "  for i in range(len(datatoClassify)) :\n",
        "\n",
        "    tokenized = tokenizer(str(datatoClassify[i]), return_tensors=\"np\", padding=\"longest\", truncation= True)\n",
        "    \n",
        "    ''' it saves the tokenized form of the model output and takes the maximum value in the matirx along axis one, \n",
        "        which is equal to the output label''' \n",
        "    outputs = model(tokenized).logits\n",
        "    classificationsEncoded = np.argmax(outputs, axis=1)\n",
        "\n",
        "    #to convert the id of the label to its string privacy aspect\n",
        "    classifications = [model.config.id2label[output] for output in classificationsEncoded]\n",
        "    \n",
        "    # only the aspects corresponding to the users privacy aspects are saved \n",
        "    if int(classificationsEncoded) in user:\n",
        "      \n",
        "      if int(classificationsEncoded) not in result.keys():\n",
        "       result[int(classificationsEncoded)]= []\n",
        "      \n",
        "      result[int(classificationsEncoded)].append(datatoClassify[i])\n",
        "  \n",
        "  return result "
      ],
      "metadata": {
        "id": "Vf1PigIGUVwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Segment Summarization"
      ],
      "metadata": {
        "id": "Ecqu3VFfdR9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To split the input into sentences and then tokenize into separate words\n",
        "\n",
        "def segmentToTokens(segments):\n",
        "  temp2 = [word_tokenize(t) for t in sent_tokenize(segments)]\n",
        "  return temp2"
      ],
      "metadata": {
        "id": "rJRyT-onekuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to calculate the frequency of each word in a segment, vectorise it and then find the cosine distance between pairs of vectories sentences.\n",
        "def sentenceSimilarity(s1, s2, stopWords=None):\n",
        "\n",
        "  #to avoid counting the stop words\n",
        "  if stopWords is None:\n",
        "    stopWords = []\n",
        "  s1 = [wd.lower() for wd in s1]    \n",
        "  s2 = [wd.lower() for wd in s2]     \n",
        "  \n",
        "  wordList = list(set(s1 + s2))    \n",
        "  vector1 = [0] * len(wordList)    \n",
        "  vector2 = [0] * len(wordList)     \n",
        "  \n",
        "  #to build the vector for the first sentence   \n",
        "  for word in s1:\n",
        "    if word in stopWords:\n",
        "      continue        \n",
        "    vector1[wordList.index(word)] += 1     \n",
        "\n",
        "  # to  build the vector for the second sentence    \n",
        "  for word in s2:        \n",
        "    if word in stopWords:           \n",
        "      continue        \n",
        "    vector2[wordList.index(word)] += 1     \n",
        "  \n",
        "  return 1 - cosine_distance(vector1,  vector2)"
      ],
      "metadata": {
        "id": "dH9GrQUohcnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to generate a matrix containing similarity scores\n",
        "\n",
        "def similarityMatrix(sentences, stopWords):\n",
        "  similarityMtrx = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "      if not i == j:\n",
        "        similarityMtrx[i][j] = sentenceSimilarity(sentences[i], sentences[j], stopWords)\n",
        " \n",
        "  return similarityMtrx"
      ],
      "metadata": {
        "id": "BPk6FdfBp4sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main summarization function\n",
        "\n",
        "def summarizer(sentences, length):\n",
        "  #defines the list of stop words to be used\n",
        "  stopWords = stopwords.words('english')\n",
        "  #a list to save the summarized sentences\n",
        "  summarizedText = []\n",
        "  \n",
        "  similarityMtrx = similarityMatrix(sentences, stopWords)\n",
        "  \n",
        "  #changing the two dimensional matrix to an array\n",
        "  similarityMaxArray = nx.from_numpy_array(similarityMtrx)\n",
        "  \n",
        "  # to store the sentences with max similariyt scores form the pair of sentences\n",
        "  scores = nx.pagerank(similarityMaxArray)\n",
        "  \n",
        "  # to arrange to scores in descending order\n",
        "  ranks = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "  \n",
        "  #uncoment to view the similary scores of the selected sentences\n",
        "  #print(\"Indexes of top rank order are \", ranks)\n",
        "  for i in range(length):\n",
        "    if ranks[i][1] != ranks[(i+1)][1]:\n",
        "      summarizedText.append(\" \".join(ranks[i][1]))\n",
        "  \n",
        "  #to combine the list of sentences into a string\n",
        "  summary = \" \".join(summarizedText)\n",
        "  \n",
        "  return summary"
      ],
      "metadata": {
        "id": "WKL1V-rMg1Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to display the final output\n",
        "def displayOutput(opt, user):\n",
        "  \n",
        "  id2Label = {0: 'Data Retention', 1: 'Data Security', 2: 'Do Not Track', 3: 'First Party Collection/Use',\n",
        "              4: 'International and Specific Audiences', 5: 'Introductory/Generic', 6: 'Policy Change',\n",
        "              7: 'Practice not covered', 8:'Privacy contact information', 9: 'Third Party Sharing/Collection',\n",
        "              10: 'User Access, Edit and Deletion', 11: 'User Choice/Control'}\n",
        "\n",
        "  print(user.upper() + \"\\n\")\n",
        "\n",
        "  userType = customizer(user)\n",
        "\n",
        "  for key in opt.keys():\n",
        "    print(key)\n",
        "    sentences =' '.join(opt[key].split())\n",
        "    sentences = sent_tokenize(sentences)\n",
        "    for s in sentences:\n",
        "      print(s)\n",
        "    print(\"\\n\")\n",
        "  \n",
        "  for k1 in userType:\n",
        "    if id2Label[k1] not in opt.keys():\n",
        "      print(id2Label[k1] + \"\\nNo data present.\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MKrikzTu2Cnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to count the number of words in raw scraped data of privacy policy and the final summary of the privacy policy\n",
        "def countWords(stage, content):\n",
        "  count = 0\n",
        "  if stage == \"ini\":\n",
        "    for i in content:\n",
        "      count = count + len(i.split())\n",
        "  else :  \n",
        "    for key in content.keys():\n",
        "      count = count + len(opt[key].split())\n",
        "  \n",
        "  return count"
      ],
      "metadata": {
        "id": "fJI10Gz_8m_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main"
      ],
      "metadata": {
        "id": "wurzstiwrJpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 1\n",
        "\n",
        "#please input a user type  among : \"young male\", \"old male\", \"young female\", \"old female\"\n",
        "user = \"young female\"\n",
        "\n",
        "#please enter the url of the privacy policy. Few example urls are commented below\n",
        "\n",
        "#url = \"https://huggingface.co/privacy\"\n",
        "#url = \"https://stackoverflow.com/legal/privacy-policy\"\n",
        "#url = \"https://policies.google.com/privacy?\"\n",
        "#url = \"https://www.linkedin.com/legal/privacy-policy\"  \n",
        "#url = \"https://meta.wikimedia.org/wiki/Privacy_policy\"\n",
        "#url = \"https://www.apple.com/legal/privacy/en-ww/\" \n",
        "#url = \"https://www.samsung.com/au/info/privacy/\"\n",
        "#url = \"https://legal.yahoo.com/au/en/yahoo/privacy/products/searchservices/index.html\"\n",
        "url = \"https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement\""
      ],
      "metadata": {
        "id": "dr2Lelcq-ftR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 2\n",
        "\n",
        "#to scrape the url\n",
        "\n",
        "rawData = scrapeUrl(url)"
      ],
      "metadata": {
        "id": "A-TzZGzYrCWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 3\n",
        "\n",
        "#to preprocess the scraped data\n",
        "dataForClassification = minPreProcess(rawData)"
      ],
      "metadata": {
        "id": "ruMrIr7M6RQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 4\n",
        "\n",
        "#to classify the privacy policy and saved it as dictionary\n",
        "dataAfterClassification = dict()\n",
        "dataAfterClassification = classificationModel(dataForClassification, user)"
      ],
      "metadata": {
        "id": "BNzj7QJs35N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 5\n",
        "\n",
        "#to store the classified segments as dictionary\n",
        "\n",
        "joinedData = storeAsDict(dataAfterClassification)"
      ],
      "metadata": {
        "id": "LyfgxSPH399s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joinedData.keys()"
      ],
      "metadata": {
        "id": "SOvh2sD1X84Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joinedData['Data Security']"
      ],
      "metadata": {
        "id": "IFntQ1jkYD5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 6\n",
        "\n",
        "# to summarize the classified segments\n",
        "\n",
        "opt = dict()\n",
        "for key in joinedData.keys():\n",
        "  seg = segmentToTokens(joinedData[key])\n",
        "  if len(seg) > 5:\n",
        "    opt[key] = summarizer(seg,5)\n",
        "  else :\n",
        "    opt[key] = joinedData[key]\n"
      ],
      "metadata": {
        "id": "nmUXbd36kYN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 7\n",
        "\n",
        "# to display the final summary\n",
        "\n",
        "displayOutput(opt, user)"
      ],
      "metadata": {
        "id": "GNmEIFEzfbp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step - 8\n",
        "\n",
        "# to display the number of words and compression ratio\n",
        "\n",
        "initialCount = countWords(\"ini\", rawData)\n",
        "finalCount = countWords(\"\", opt)\n",
        "print(\"Initial count = \" + str(initialCount))\n",
        "print(\"\\nFinal count = \" + str(finalCount))\n",
        "print(\"\\nCompression ration = {cr:0.4f}\".format(cr=finalCount/initialCount))"
      ],
      "metadata": {
        "id": "X7Vey6Yv44SC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}