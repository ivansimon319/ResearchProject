{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing and importing libraries"
      ],
      "metadata": {
        "id": "_lvadF8JUcYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLLILvikB5Lb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#Required Libraries\n",
        "\n",
        "#import os\n",
        "import torch\n",
        "import string\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import create_optimizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import load_dataset, Dataset, load_from_disk\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import TFAutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "zQxOPbOqCCJT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "#To mount the google drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "metadata": {
        "id": "uhUyi-wOCnoz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main code to train the model"
      ],
      "metadata": {
        "id": "1qYpzKr2UqbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path where the training data is saved\n",
        "\n",
        "#change the path to the file path where the dataset is saved\n",
        "trainPath = \"/content/drive/MyDrive/train_dataset.csv\"\n",
        "valPath = \"/content/drive/MyDrive/validation_dataset.csv\"\n",
        "\n",
        "#change the path to the path where Dataframe could be saved\n",
        "tDFPath = '/content/drive/MyDrive/OPPtrainDataframe'\n",
        "valDFPath = '/content/drive/MyDrive/OPPvalidationDataFrame'\n",
        "\n",
        "# Uncomment if test dataset is used.\n",
        "# testPath = \"/content/drive/MyDrive/test_dataset.csv\"\n",
        "# testDFPath = '/content/drive/MyDrive/OPPtestDataFrame'"
      ],
      "metadata": {
        "id": "KKMzdeXdCqcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To save the datasets as Pandas dataframes\n",
        "\n",
        "trainData = pd.read_csv(trainPath)\n",
        "valData = pd.read_csv(valPath)\n",
        "\n",
        "# Uncomment if test dataset is used\n",
        "#testData = pd.read_csv(testPath)"
      ],
      "metadata": {
        "id": "lXmzp98SCM41"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To rename the columns of the dataframes containing the training, testing and validation data\n",
        "trainData.columns = [\"Sentence\", \"label\"]\n",
        "valData.columns = [\"Sentence\", \"label\"]\n",
        "\n",
        "# Uncomment if test dataset is used\n",
        "# testData.columns = [\"Sentence\", \"label\"]"
      ],
      "metadata": {
        "id": "E6tyjLHENVic"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To view the labels present in the training dataset\n",
        "trainData['label'].unique()"
      ],
      "metadata": {
        "id": "IPkCqG0AC3Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To display the number of sample in the training, validation and testing dataset samples\n",
        "\n",
        "print (\"Training Dataset = \" + str(len(trainData)) + \"  Validation Dataset = \" + str(len(valData)))"
      ],
      "metadata": {
        "id": "OHAYScmhu4Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To view a sample of the train data\n",
        "trainData[490:491]"
      ],
      "metadata": {
        "id": "tnHbI_W-qsZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''To save the Pandas dataframe in .pkl format.This data would then be loaded with the help of transformer library's function \n",
        "   to create a dataframe combining both training and validation sets under different heads'''\n",
        "    \n",
        "trainData.to_pickle(tDFPath)\n",
        "valData.to_pickle(valDFPath)\n",
        "\n",
        "# testData.to_pickle(testDFPath)"
      ],
      "metadata": {
        "id": "nrvRwwRfC8ne"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionry containing path of the training and validation dataset\n",
        "\n",
        "fDataset= {\"train\" : tDFPath, \"val\" : valDFPath}"
      ],
      "metadata": {
        "id": "2toepDQXC-84"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the training and validation dataset as one Dataframe with two heads, making it compatible to be used with tranformer library models\n",
        "\n",
        "transformerDataset = load_dataset(\"pandas\", data_files=fDataset)"
      ],
      "metadata": {
        "id": "I6t1oPgsDmsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to encode the string labels in interger form \n",
        "\n",
        "transformerDataset = transformerDataset.class_encode_column(\"label\")"
      ],
      "metadata": {
        "id": "pi0Ue3CK6eiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing tasks\n",
        "\n",
        "def toLowercase(example):\n",
        "    return {\"Sentence\": example[\"Sentence\"].lower()}\n",
        "\n",
        "transformerDataset = transformerDataset.map(toLowercase)\n",
        "\n",
        "def removePunctuations(example):\n",
        "  return{\"Sentence\": example[\"Sentence\"].translate(str.maketrans('','',string.punctuation))}\n",
        "\n",
        "transformerDataset = transformerDataset.map(removePunctuations)"
      ],
      "metadata": {
        "id": "qNDlzNPXHVa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample view after the preporcessing task\n",
        "transformerDataset[\"val\"][201:202]"
      ],
      "metadata": {
        "id": "s9Xs7xtOHa8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To import tokenizer for PrivBERT model from the HuggingFace Library\n",
        "\n",
        "checkpoint = \"mukund/privbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "xSS0-2uBIeog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function to perform tokenization via callback function of tranformer dataset\n",
        "\n",
        "def tokenizeFunction(example):\n",
        "    return tokenizer(example[\"Sentence\"], example[\"Sentence\"],truncation=True)"
      ],
      "metadata": {
        "id": "6cedGA9LJxbK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It uses the tokenizeFunction and performs tokenization batchwise over the entire dataset\n",
        "\n",
        "encodedDataset = transformerDataset.map(tokenizeFunction, batched=True)"
      ],
      "metadata": {
        "id": "nrIDpoGsKBfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to view the features added by the tokenizer (the input_ids and attention_mask)\n",
        "\n",
        "encodedDataset[\"train\"].features"
      ],
      "metadata": {
        "id": "lbRTuMwiKTb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To remove unwanted columns and rename the column to names accepted by the model\n",
        "\n",
        "encodedDataset = encodedDataset.remove_columns([\"Sentence\"])\n",
        "encodedDataset = encodedDataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "WgBgoS_8l48t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "OupapIYmZIhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To import the pre-trained PrivBERT model from the HuggingFace Library\n",
        "\n",
        "#number of label is equal to the privacy aspects present in the OPP-115 dataset\n",
        "numLabels = 12\n",
        "    \n",
        "# For cleaner label outputs, mapping is given from encoded label to actual label names\n",
        "id2label = {0: 'Data Retention', 1: 'Data Security', 2: 'Do Not Track', 3: 'First Party Collection/Use',\n",
        "            4: 'International and Specific Audiences', 5: 'Introductory/Generic', 6: 'Policy Change',\n",
        "            7: 'Practice not covered', 8:'Privacy contact information', 9: 'Third Party Sharing/Collection',\n",
        "            10: 'User Access, Edit and Deletion', 11: 'User Choice/Control'}\n",
        "\n",
        "label2id = {val: key for key, val in id2label.items()}\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint, num_labels=numLabels, id2label=id2label, label2id=label2id)"
      ],
      "metadata": {
        "id": "pcoEpdxDKYOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To provide padding to the tenfors formed, in the next step, to avoid unmatched dimensions\n",
        "\n",
        "dataCollator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "3DmmJqEQRt5d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to form a tensor of the training and validation dataset to be fed to the model with a batch size of 12, tokenizer and data padding\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    encodedDataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=12,\n",
        "    collate_fn=dataCollator,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "tf_validation_dataset = model.prepare_tf_dataset(\n",
        "    encodedDataset[\"val\"],\n",
        "    shuffle=True,\n",
        "    batch_size=12,\n",
        "    collate_fn=dataCollator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "E0H67vJKZ_Zd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the parameter to train the model\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 12\n",
        "batches_per_epoch = len(encodedDataset[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "\n",
        "# an optimiser to provide the hyperparameters\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2.5e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "879Jrz2nah7z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the metric system from the tranformer library \n",
        "metric = load_metric(\"accuracy\")"
      ],
      "metadata": {
        "id": "9VgQY1Vebwva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a function to define on what parameters the metrics should be applied\n",
        "\n",
        "def computeMetrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "xo5sYDZDb2Fj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a keras callback function to calculate the metrics on the validation set after each epoch\n",
        "\n",
        "metricCallback = KerasMetricCallback(\n",
        "    metric_fn=computeMetrics, eval_dataset=tf_validation_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "hG_zoOCNcElb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To train the model\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=metricCallback,\n",
        ")"
      ],
      "metadata": {
        "id": "xW1JKRcDm4S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "veVjucT1k7Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to save the configuration of the trained model\n",
        "\n",
        "model_dir = '/content/drive/My Drive/Colab Notebooks/models/' #change the path to the folder where you want to save th configuration\n",
        "\n",
        "model.save_pretrained(model_dir + 'PolicyInterpreterFullSample')"
      ],
      "metadata": {
        "id": "-teRLYUZnTr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}